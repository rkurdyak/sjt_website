# -*- coding: utf-8 -*-
"""open_source_toronto_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X8jJQmuioQvGoDLY2SbYMqi0wm1A4TDc
"""

# create a requirements 
# pip install geopandas folium

import requests
import os
import json
import pandas as pd

# Define the base URL for the City of Toronto CKAN API
BASE_URL = "https://ckan0.cf.opendata.inter.prod-toronto.ca"

def fetch_package(package_id, base_url=BASE_URL):
    """
    Fetch the package (dataset) metadata using its ID.
    """
    url = f"{base_url}/api/3/action/package_show"
    params = {"id": package_id}
    response = requests.get(url, params=params)
    response.raise_for_status()
    return response.json()["result"]

def fetch_resource_data(resource, base_url=BASE_URL):
    """
    Fetch resource data depending on whether it is datastore active or not.
    Returns either raw CSV/text or binary content for download.
    """
    if resource.get("datastore_active"):
        # Option 1: Retrieve full CSV dump
        dump_url = f"{base_url}/datastore/dump/{resource['id']}"
        csv_data = requests.get(dump_url).text
        return csv_data, "csv"
    else:
        # Option 2: Retrieve resource metadata to get download URL
        meta_url = f"{base_url}/api/3/action/resource_show"
        params = {"id": resource["id"]}
        meta_response = requests.get(meta_url, params=params)
        meta_response.raise_for_status()
        resource_metadata = meta_response.json()["result"]
        download_url = resource_metadata.get("url")
        file_response = requests.get(download_url)
        file_response.raise_for_status()
        # Infer format from metadata or URL extension
        file_format = resource_metadata.get("format", "").lower() or os.path.splitext(download_url)[-1].lstrip(".")
        return file_response.content, file_format

"""## Load Data from API"""

def save_resource(data, file_format, filename):
    """
    Save the downloaded resource data to a file.
    """
    mode = "w" if file_format == "csv" else "wb"
    with open(filename, mode) as f:
        f.write(data)
    print(f"Saved file to {filename}")

# Example usage:
# data, file_format = fetch_resource_data(resource)
# save_resource(data, file_format, f"{resource['id']}.{file_format}")

def load_csv(filename):
    """
    Load a CSV file into a pandas DataFrame.
    """
    return pd.read_csv(filename)

def load_json(filename):
    """
    Load a JSON file.
    """
    with open(filename, "r") as f:
        return json.load(f)

# Example usage:
# df = load_csv("neighbourhood_profiles.csv")
# print(df.head())

import os.path
from os import path

def process_package(package_id):
    # Fetch package metadata
    package = fetch_package(package_id)
    if not path.exists(f'/content/{package_id}'):
        os.mkdir(f'/content/{package_id}')
    os.chdir(f'/content/{package_id}')
    # Iterate over resources
    for resource in package.get("resources", []):
        print(f"Processing resource: {resource.get('name')}")
        try:
            data, file_format = fetch_resource_data(resource)
            # Create a safe filename based on resource id and file format
            filename = f"{resource['name']}.{file_format}"
            save_resource(data, file_format, filename)

            # Load data if it's in CSV format
            if file_format == "csv":
                df = load_csv(filename)
                print(f"Loaded {len(df)} rows from {filename}")
            # Add additional loaders if needed
        except Exception as e:
            print(f"Failed to process resource {resource.get('id')}: {e}")

# Example: Process the 'neighbourhood-profiles' package
process_package("neighbourhood-profiles")

process_package('neighbourhoods')

# prompt: Based on the above code, how can I get information about the files I'm downloading? Is there any coherent way I should be filtering for files?

import requests
import os
import json
import pandas as pd
import os.path
from os import path
!pip install geopandas folium


# Define the base URL for the City of Toronto CKAN API
BASE_URL = "https://ckan0.cf.opendata.inter.prod-toronto.ca"

def fetch_package(package_id, base_url=BASE_URL):
    """
    Fetch the package (dataset) metadata using its ID.
    """
    url = f"{base_url}/api/3/action/package_show"
    params = {"id": package_id}
    response = requests.get(url, params=params)
    response.raise_for_status()
    return response.json()["result"]

def fetch_resource_data(resource, base_url=BASE_URL):
    """
    Fetch resource data depending on whether it is datastore active or not.
    Returns either raw CSV/text or binary content for download.
    """
    if resource.get("datastore_active"):
        # Option 1: Retrieve full CSV dump
        dump_url = f"{base_url}/datastore/dump/{resource['id']}"
        csv_data = requests.get(dump_url).text
        return csv_data, "csv"
    else:
        # Option 2: Retrieve resource metadata to get download URL
        meta_url = f"{base_url}/api/3/action/resource_show"
        params = {"id": resource["id"]}
        meta_response = requests.get(meta_url, params=params)
        meta_response.raise_for_status()
        resource_metadata = meta_response.json()["result"]
        download_url = resource_metadata.get("url")
        file_response = requests.get(download_url)
        file_response.raise_for_status()
        # Infer format from metadata or URL extension
        file_format = resource_metadata.get("format", "").lower() or os.path.splitext(download_url)[-1].lstrip(".")
        return file_response.content, file_format

# ## Load Data from API

def save_resource(data, file_format, filename):
    """
    Save the downloaded resource data to a file.
    """
    mode = "w" if file_format == "csv" else "wb"
    with open(filename, mode) as f:
        f.write(data)
    print(f"Saved file to {filename}")

# Example usage:
# data, file_format = fetch_resource_data(resource)
# save_resource(data, file_format, f"{resource['id']}.{file_format}")

def load_csv(filename):
    """
    Load a CSV file into a pandas DataFrame.
    """
    return pd.read_csv(filename)

def load_json(filename):
    """
    Load a JSON file.
    """
    with open(filename, "r") as f:
        return json.load(f)

# Example usage:
# df = load_csv("neighbourhood_profiles.csv")
# print(df.head())


def process_package(package_id):
    # Fetch package metadata
    package = fetch_package(package_id)
    if not path.exists(f'/content/{package_id}'):
        os.mkdir(f'/content/{package_id}')
    os.chdir(f'/content/{package_id}')
    # Iterate over resources
    for resource in package.get("resources", []):
        print(f"Processing resource: {resource.get('name')}")
        try:
            data, file_format = fetch_resource_data(resource)
            # Create a safe filename based on resource id and file format
            filename = f"{resource['name']}.{file_format}"
            save_resource(data, file_format, filename)

            # Print resource information
            print(f"  - Resource ID: {resource.get('id')}")
            print(f"  - Resource Name: {resource.get('name')}")
            print(f"  - Resource Format: {file_format}")
            print(f"  - Resource Description: {resource.get('description')}") # Added description
            print(f"  - Resource URL: {resource.get('url')}") # Added URL


            # Load data if it's in CSV format
            if file_format == "csv":
                df = load_csv(filename)
                print(f"Loaded {len(df)} rows from {filename}")
            # Add additional loaders if needed
        except Exception as e:
            print(f"Failed to process resource {resource.get('id')}: {e}")

# Example: Process the 'neighbourhood-profiles' package
process_package("neighbourhood-profiles")
process_package('neighbourhoods')

import geopandas as gpd
neighbourhoods = gpd.read_file(r'/content/neighbourhoods/Neighbourhoods - 4326.geojson.geojson')

neighbourhoods

neighbourhoods['AREA_SHORT_CODE'] = neighbourhoods['AREA_SHORT_CODE'].astype(int)
neighbourhoods_relevant_columns = neighbourhoods[['AREA_SHORT_CODE', 'AREA_NAME', 'geometry']]

neighbourhoods.shape

"""## Neighborhood Profile Data
  - Prepare data to match census question hierarchy
  - prepare main topics, and group data by topic to make it easier to answer questions
"""



neighbourhood_profiles_2021 = pd.read_excel(r'/content/neighbourhood-profiles/neighbourhood-profiles-2021-158-model.xlsx')
neighbourhood_profiles_2021



import pandas as pd

# Assuming your data is in a CSV file

# Create a function to calculate indentation level
def get_indent_level(text):
    return len(text) - len(text.lstrip())

# Apply the function to the relevant column (e.g. 'question')
neighbourhood_profiles_2021['indent_level'] = neighbourhood_profiles_2021['Neighbourhood Name'].apply(get_indent_level)
neighbourhood_profiles_2021['clean_question'] = neighbourhood_profiles_2021['Neighbourhood Name'].str.strip()

main_topic = None
topics = []

for level, question in zip(neighbourhood_profiles_2021['indent_level'], neighbourhood_profiles_2021['clean_question']):
    if level == 0:  # assuming main topics have no indent
        main_topic = question
    topics.append(main_topic)

neighbourhood_profiles_2021['main_topic'] = topics

neighbourhood_profiles_2021['main_topic'].unique()

neighbourhood_profiles_2021[neighbourhood_profiles_2021['indent_level'] == 16]

# Identify rows with no leading spaces
no_leading_spaces = neighbourhood_profiles_2021.apply(lambda row: row.str.startswith(' ').all(), axis=1)
neighbourhood_profiles_2021[~no_leading_spaces].head(10)

neighbourhood_profiles_2021['main_topic'] = neighbourhood_profiles_2021['main_topic'].str.replace('Total - ', '')

base_url = "https://ckan0.cf.opendata.inter.prod-toronto.ca"

# Datasets are called "packages". Each package can contain many "resources"
# To retrieve the metadata for this package and its resources, use the package name in this page's URL:
url = base_url + "/api/3/action/package_show"
params = { "id": "fire-incidents"}
package = requests.get(url, params = params).json()

for idx, resource in enumerate(package["result"]["resources"]):
  print(resource)

neighbourhood_profiles_2021_t = neighbourhood_profiles_2021.transpose()

new_headers = neighbourhood_profiles_2021_t.iloc[0]
neighbourhood_profiles_2021_t = neighbourhood_profiles_2021_t[1:]
neighbourhood_profiles_2021_t.columns = new_headers
neighbourhood_profiles_2021_t.head()

neighbourhood_profiles_2021_t

neighbourhoods_relevant_columns

merged = neighbourhoods_relevant_columns.merge(neighbourhood_profiles_2021_t, how = 'right', left_on="AREA_SHORT_CODE", right_on="Neighbourhood Number").iloc[:-3,:]
age_statistics = merged.iloc[:,:31]

neighbourhoods_relevant_columns

merged[['AREA_SHORT_CODE','AREA_NAME']]

# Basic example for
total_population = age_statistics[['AREA_SHORT_CODE','Total - Age groups of the population - 25% sample data']]

merged.columns = merged.columns.str.strip()
merged.columns



# 1. Extract the set of GeoJSON names
geo_names = {
    feature['properties']['AREA_SHORT_CODE']
    for feature in neighbourhoods_relevant_columns.__geo_interface__['features']
}

# 2. Extract the set of your data names
data_names = set(total_population['AREA_SHORT_CODE'])

# 3. See which names are unmatched
missing = data_names - geo_names
print("These neighbourhoods won’t map:", missing)

merged['AREA_NAME']

total_population['AREA_SHORT_CODE'] = total_population['AREA_SHORT_CODE'].astype(int)

neighbourhoods_relevant_columns.info()

neighbourhoods_relevant_columns.__geo_interface__['features'][0]['properties']['AREA_SHORT_CODE']

# GeoJSON IDs
geo_ids = set(feature['properties']['AREA_SHORT_CODE'] for feature in neighbourhoods_relevant_columns.__geo_interface__['features'])

# DataFrame values
df_ids = set(total_population['AREA_SHORT_CODE'])

# Any mismatches?
print("Only in GeoJSON:", geo_ids - df_ids)
print("Only in DataFrame:", df_ids - geo_ids)

neighbourhoods.__geo_interface__['features'][0]['properties']



merged.columns

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10,10))
merged.plot(
    column="Total - Age groups of the population - 25% sample data",
    ax=ax,
    legend=True,
    legend_kwds={"title": "Population by Neighbourhood"},
    scheme="Quantiles",
    k=5
)
ax.set_title("Toronto Population by Neighbourhood (2016)")
ax.axis("off")
plt.show()



# assume `df` has a MultiIndex on columns
flat_cols = [
    " – ".join([lvl for lvl in col if lvl])  # join levels with a separator
    for col in merged.columns
]
df_flat = merged.copy()
df_flat.columns = flat_cols
df_flat

# Reset the index if needed (e.g. neighbourhood identifier)
df_reset = merged.reset_index()
df_reset

# Melt: one row per (neighbourhood, question‑levels, value)
long = df_reset.melt(
    id_vars=['Neighbourhood Number'],       # your unique key
    #var_name=['Level_0','Level_1','Level_2'],  # names for each hierarchy level
    value_name='Count'
)
long

"""## Making the MultiIndex
- Create a MultiIndex to allow for users to slice the dataset by question.
"""

# suppose census_df has columns: ['Neighbourhood', 'Total - Age groups of the population', '0 to 14 years', …]

neighbourhood_questions = neighbourhood_profiles_2021_t.columns.tolist()

# count leading spaces, then infer level (2 spaces = one indent)
levels = [(len(c) - len(c.lstrip())) // 2 for c in neighbourhood_questions]
max_level = max(levels)  # e.g. 2

# strip names
names = [c.strip() for c in neighbourhood_questions]

# track the “current” header at each level
current = {}
tuples = []

for lvl, name in zip(levels, names):
    current[lvl] = name
    # discard any deeper levels from previous iterations
    for deeper in [k for k in current if k > lvl]:
        current.pop(deeper)
    # build a list of length max_level+1, filling missing with ''
    lsts = [current.get(i, '') for i in range(max_level+1)]
    # if the string begins with 'Total - ', remove it from the start of the string
    if lsts[0].startswith('Total - '):
        lsts[0] = lsts[0].replace('Total - ', '')
    tup = tuple(lsts)
    tuples.append(tup)

# create the MultiIndex
mi = pd.MultiIndex.from_tuples(
    tuples,
    names=[f'Level_{i}' for i in range(max_level+1)]
)

mi

neighbourhood_profiles_2021_t.columns = mi
neighbourhood_profiles_2021_t.head()

neighbourhood_profiles_2021_t = neighbourhood_profiles_2021_t.set_index(('Neighbourhood Number','', ''))  # top‑level only
neighbourhood_profiles_2021_t.index.name = 'Neighbourhood Number'

# Select topic and subtopic, choose no index level names
ages_0_14 = neighbourhood_profiles_2021_t[('Total - Age groups of the population - 25% sample data',
               '0 to 14 years','')]
ages_0_14.head()

for level in neighbourhood_profiles_2021_t.columns.levels:
  print(level.name)

# Suppose df has a 3‑level column MultiIndex
lvl0 = neighbourhood_profiles_2021_t.columns.levels[0]   # e.g. ['Total - Age groups…', 'Total - Income brackets…', …]
lvl1 = neighbourhood_profiles_2021_t.columns.levels[1]   # e.g. ['0 to 14 years', '15 to 64 years', …]
lvl2 = neighbourhood_profiles_2021_t.columns.levels[2]   # often ['', '0 to 4 years', …]
lvl3 = neighbourhood_profiles_2021_t.columns.levels[3]
lvl4 = neighbourhood_profiles_2021_t.columns.levels[4]
lvl5 = neighbourhood_profiles_2021_t.columns.levels[5]
lvl6 = neighbourhood_profiles_2021_t.columns.levels[6]
lvl7 = neighbourhood_profiles_2021_t.columns.levels[7]

lvl7

from itertools import product

# Only include combinations that actually exist in your DataFrame
valid = set(neighbourhood_profiles_2021_t.columns)  # a set of all real tuples

# Generate all possible tuples…
candidates = product(lvl0, lvl1, lvl2)

# …but keep only the ones in your actual columns
selection_tuples = [t for t in candidates if t in valid]

subs = neighbourhood_profiles_2021_t.columns[neighbourhood_profiles_2021_t.columns.get_level_values(0)=='Age groups of the population - 25% sample data']#[neighbourhood_profiles_2021_t.columns.get_level_values(0)==questi]

subquestions = subs.get_level_values(1).unique()
subquestions
question = 'Age groups of the population - 25% sample data'
tuples_for_question = [(question, sub, '') for sub in subquestions]

def get_all_subquestions(df, top_level_question, sublevels = [1]):

  subs = df.columns[df.columns.get_level_values(0)==question]
  sublevel_dict = {}
  for sublevel in sublevels:

  subs = subs.get_level_values(sublevels[0]).unique()
  tuples_for_question = [(question, sub, '') for sub in subs]
  return tuples_for_question



age_questions = get_all_subquestions(neighbourhood_profiles_2021_t, 'Age groups of the population - 25% sample data')

age_questions



"""## Graphing"""

neighbourhood_profiles_2021_t

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10,10))
merged.plot(
    column="Population",
    ax=ax,
    legend=True,
    legend_kwds={"label": "Population by Neighbourhood"},
    scheme="Quantiles",
    k=5
)
ax.set_title("Toronto Population by Neighbourhood (2016)")
ax.axis("off")
plt.show()

import geopandas as gpd

geojson_url = (
    "https://raw.githubusercontent.com/jasonicarter/toronto-geojson"
    "/master/toronto_crs84.geojson"
)
neigh_gdf = gpd.read_file(geojson_url)
neigh_gdf.head()



neigh_gdf['AREA_S_CD'] = neigh_gdf['AREA_S_CD'].astype(int)
neigh_gdf.head()

# TODO: get rid of the non-numeric values
neigh_gdf['AREA_S_CD'] = neigh_gdf['AREA_S_CD'].astype(int)
neigh_gdf.head()


neigh_pr_graphing = neighbourhood_profiles_2021_t.copy().iloc[:-3]

neigh_pr_graphing